{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GPU_test.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPKw9xUqJjLnxZeLg5mu0cl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZVxA2_TY1V3K"},"source":["!pip3 list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yw2QgqsxCyN_"},"source":["import json\n","import time\n","#from numba import jit\n","import openpyxl\n","import os\n","import torch\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UpsE2f6VC2MF"},"source":["os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  #選擇gpu或cpu\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xehy-tQQDrSF"},"source":["def write_excel_xlsx(path, sheet_name, value):\n","    index = len(value)\n","    workbook = openpyxl.Workbook()\n","    sheet = workbook.active\n","    sheet.title = sheet_name\n","    for i in range(0, index):\n","        for j in range(0, len(value[i])):\n","            sheet.cell(row=i+1, column=j+1, value=str(value[i][j]))\n","    workbook.save(path)\n","\n","workbook = None\n","q_list = []\n","a_list = []\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PLOG--9jU3yd"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFn0R7jvZAGP"},"source":["import os \n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/Bert_project_finetuned_V4(多線程,成功)')\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KDKS_IblW1RA"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cjQrXkfuTFHg"},"source":["from google.colab import files\n","import json\n","x = json.dumps ('test')\n","y = x.encode('utf-8')\n","def read_corpus():\n","    \"\"\"\n","    读取给定的语料库，并把问题列表和答案列表分别写入到 qlist, alist 里面。 在此过程中，不用对字符换做任何的处理（这部分需要在 Part 2.3里处理）\n","    qlist = [\"问题1\"， “问题2”， “问题3” ....]\n","    alist = [\"答案1\", \"答案2\", \"答案3\" ....]\n","    务必要让每一个问题和答案对应起来（下标位置一致）\n","    \"\"\"\n","    # TODO 需要完成的代码部分 ...\n","    \n","    # 读取question的json\n","    with open('q_list.json', 'r') as file:\n","        question = file.read()\n","        qlist = json.loads(question)\n","\n","    # 读取answers的json\n","    with open('a_list.json', 'r') as file:\n","        answers = file.read()\n","        alist = json.loads(answers)\n","    \n","    assert len(qlist) == len(alist)  # 确保长度一样\n","    return qlist, alist\n","\n","corpus=read_corpus()\n","qlist=corpus[0]\n","alist=corpus[1]\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rrnh5fWoU2tP"},"source":["import time\n","def Read_stopwords():\n","    start=time.time()\n","    fb=open(\"stopwords/stopwords.txt\",\"r\",encoding='utf-8')\n","    stopwords_list=[]\n","    for stopwords in fb.readlines():\n","        stopwords=stopwords.strip(\"\\n\")\n","        stopwords_list.append(stopwords)\n","    print(\"elapsed time of python:\",time.time()-start)\n","    return stopwords_list\n","stopwords_list=Read_stopwords()\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYSB4R3UdD3o"},"source":["def movestopwords(sentence):\n","    #from nltk.corpus import stopwords\n","    #from nltk.tokenize import word_tokenize\n","    import jieba\n","    q_filtered_list = []\n","    # 将每一个问题取出来\n","    for question in sentence:\n","        \n","        # 把问题拆解成一个一个单词\n","        word_tokens = list(jieba.cut_for_search(question))\n","        \n","        \n","        # 过滤掉停用词\n","        ## 增加stopwords\n","        #stop_words = set(stopwords.words('chinese'))\n","        #stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n","        #filtered_words = [word for word in word_tokens if word not in stop_words]\n","        filtered_words =[word for word in word_tokens if word not in stopwords_list]\n","        #print(filtered_words)\n","\n","        # 将单词组合成句子\n","        filtered_sentens = ' '.join(filtered_words)\n","       \n","        q_filtered_list += [filtered_sentens]\n","\n","    return q_filtered_list\n","print(\"ok!!\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pw_rLrTf_Qr"},"source":["!pip install pytorch_pretrained_bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZVpq9Gff467"},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel\n","# 使用bert-base-uncased\n","tokenizer = BertTokenizer.from_pretrained('model') #.\\model\n","model = BertModel.from_pretrained('model') #.\\model\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r1rnLpf6nEVE"},"source":["import json\n","with open('clean_q_word_list.json','r') as flie:\n","    clean_q_word_list = flie.read()\n","    clean_q_list = json.loads(clean_q_word_list)\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7QD7sE_oxMM"},"source":["# 句子的分詞\n","def get_tokenized_text(sentence):\n","    \n","    \n","    '''time test'''\n","    #計算程序運行時間\n","    print(\"      -----------get_tokenized_text---------------    \")\n","    starttime = time.time()\n","    print(\"      starttime:\",starttime)\n","    \n","\n","    # 加入句子开头标签【CLS】和结尾标签【SEP】\n","    #marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n","    marked_text = \"[CLS] \"+str(sentence)+\" [SEP]\"\n","    # 把句子拆成多个分词\n","    tokenized_text_list = tokenizer.tokenize(marked_text)\n","    \n","    #print(tokenized_text_list)\n","    \n","    \n","    '''time test'''\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"      endtime:\",endtime)\n","    print('      time elapsed: ' , timeelapsed , ' seconds\\n')\n","    '''\n","    if workbook:\n","        global get_tokenized_text_time\n","        get_tokenized_text_time+=timeelapsed\n","    '''\n","    \n","    return tokenized_text_list\n","get_tokenized_text(\"為甚麼是你?\")\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oIgItozvpLN7"},"source":["\n","def get_Segment_ID(i,tokenized_text_list):\n","    segments_ids = [i] * len(tokenized_text_list)\n","    \n","    return segments_ids\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l0fu_O1Bpuoj"},"source":["#設定單字的ID。使用詞彙表將標記字符串（或標記序列）轉換為單個整數ID（或ID序列）。\n","def get_indexed_tokens(tokenized_text_list):\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_list)\n","    \n","    return indexed_tokens\n","#BERT PyTorch要求张量要转成torch张量 Convert inputs to PyTorch tensors\n","get_tokens_segments_tensor_time=0\n","def get_tokens_segments_tensor(i,sentence):\n","    \n","    #i=args[0]\n","    #sentence=args[1]\n","    # 取得句子分词\n","    tokenized_text = get_tokenized_text(sentence)\n","    \n","    #取得indexed_tokens,segments_ids\n","    indexed_tokens = get_indexed_tokens(tokenized_text)\n","    segments_ids = get_Segment_ID(i,tokenized_text)\n","    \n","    # 转换成pytorch张量\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","    \n","    \n","    \n","    \n","    return tokens_tensor, segments_tensors\n","\n","\n","\n","\n","#為了加快運算速度關閉梯度計算\n","## Torch.no_Grad关闭梯度计算，节省内存，并加快计算速度\n","def get_encoded_layers(tokens_tensor,segments_tensors):\n","    with torch.no_grad():\n","        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n","    return(encoded_layers)\n","get_sentence_embedding_time=0\n","def get_sentence_embedding(tokenized_text,encoded_layers):\n","    \n","   \n","    \n","    \n","    # Convert the hidden state embeddings into single token vectors\n","    # Holds the list of 12 layer embeddings for each token\n","    # Will have the shape: [# tokens, # layers, # features]\n","    token_embeddings = [] \n","    # For each token in the sentence...\n","    for token_i in range(len(tokenized_text)):\n","  \n","      # Holds 12 layers of hidden states for each token \n","        hidden_layers = [] \n","  \n","      # For each of the 12 layers...\n","        for layer_i in range(len(encoded_layers)):\n","            batch_i = 0\n","    \n","        # Lookup the vector for `token_i` in `layer_i`\n","            vec = encoded_layers[layer_i][batch_i][token_i]\n","        \n","            hidden_layers.append(vec)\n","    \n","        token_embeddings.append(hidden_layers)\n","        # Sanity check the dimensions:\n","        #print (\"Number of tokens in sequence:\", len(token_embeddings))\n","        #print (\"Number of layers per token:\", len(token_embeddings[0]))\n","        \n","    ## 特征值[number_of_tokens, 3072]，透过torch.cat把向量拼接\n","    #concatenated_last_4_layers = [torch.cat((layer[-1], layer[-2], layer[-3], layer[-4]), 0) for layer in token_embeddings]\n","    ## 特征值相加[number_of_tokens, 768]\n","    #summed_last_4_layers = [torch.sum(torch.stack(layer)[-4:], 0) for layer in token_embeddings]\n","    \n","    ## 平均每个token的倒数第二层，产生一个768长度句向量\n","    sentence_embedding = torch.mean(encoded_layers[11], 1)\n","        \n","   \n","    \n","    return sentence_embedding\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I-CobjRqtlSl"},"source":["#通過average pooling單詞向量來實現句子的向量\n","import numpy as np\n","\n","# 将句子拆成每一个词，将词向量平均建构句子的向量\n","X_bert = []\n","i = 0\n","initial_numpy = np.zeros((768,))\n","def get_tokens_segments_tensor_(i,sentence):\n","    '''time test\n","    #計算程序運行時間\n","    print(\"      -----------get_tokens_segments_tensor---------------    \")\n","    starttime = time.time()\n","    print(\"      starttime:\",starttime)\n","    '''\n","    # 取得句子分词\n","    tokenized_text = get_tokenized_text(sentence)\n","    \n","    #取得indexed_tokens,segments_ids\n","    indexed_tokens = get_indexed_tokens(tokenized_text)\n","    segments_ids = get_Segment_ID(i,tokenized_text)\n","    \n","    # 转换成pytorch张量\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","    \n","    \n","    '''time test\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"      endtime:\",endtime)\n","    print('      time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    if workbook:\n","        global get_tokens_segments_tensor_time\n","        get_tokens_segments_tensor_time+=timeelapsed\n","    '''\n","    return tokens_tensor, segments_tensors\n","for sentence in clean_q_list:\n","    # 句子的分词\n","    tokenized_text = get_tokenized_text(sentence)\n","    # BERT PyTorch要求张量要转成torch张量 Convert inputs to PyTorch tensors\n","    tokens_tensor, segments_tensors = get_tokens_segments_tensor_(i,sentence)\n","    # 取得隐藏层层数\n","    encoded_layers = get_encoded_layers(tokens_tensor,segments_tensors)\n","\n","    # 取得单词向量，取平均成句子的向量\n","    sentence_embedding = get_sentence_embedding(tokenized_text,encoded_layers)\n","    \n","    # 把torch张量转成array\n","    sentence_embedding_array = np.array(sentence_embedding)\n","\n","    #array全部组合起来\n","    initial_numpy = np.vstack((initial_numpy,sentence_embedding_array))\n","bert_vectorizer = initial_numpy\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71PBlSJLt2xL"},"source":["# bert的vectorizer存成h5py\n","import h5py\n","# Create a new file\n","f = h5py.File('bert_vectorizer.h5', 'w')\n","f.create_dataset('bert_vectorizer', data=bert_vectorizer)\n","f.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3h7cT4puE2U"},"source":["#建立倒排表\n","# 读取question的word_total_list.json\n","import json\n","with open('clean_q_word_list.json','r') as flie:\n","    clean_q_word_list = flie.read()\n","    clean_q_list = json.loads(clean_q_word_list)\n","\n","# 读取question的不重复单字word_total_list.json\n","with open('q_word_list.json','r') as flie:\n","    q_word_total_list = flie.read()\n","    word_total_list = json.loads(q_word_total_list)\n","# 大写变成小写\n","word_total = [item.lower() for item in word_total_list]\n","\n","\n","\n","invert_index=dict()\n","k = 0  # 提醒我目前已经建立到第几个数据\n","#print(word_total)\n","for b in word_total:\n","    temp=[]\n","    \n","    j = 0\n","    while j < len(clean_q_list):\n","        \n","\n","        field=clean_q_list[j]\n","        \n","        split_field=field.split()\n","        #print(split_field)\n","        \n","        if b in split_field:\n","            temp.append(j)\n","        #print(temp)\n","        j += 1\n","       \n","    k += 1\n","    if k % 10000 == 0:\n","        print(\"k:\",k)\n","    \n","    invert_index[b]=temp\n","    #print(invert[b])\n","# 全部单词(53157个单词)对应问题的字典存成json\n","import json\n","file_name_6 = 'inverted_idx.json'\n","with open(file_name_6,'w') as file_object:\n","    json.dump(invert_index,file_object)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ilzG9AcuJdl"},"source":["#(1)去除停用词\n","def q_movestopwords(sentence):\n","    #time test\n","    print(\"----------------q_movestopwords-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","    \n","    \n","    #from nltk.corpus import stopwords\n","    #from nltk.tokenize import word_tokenize\n","    import jieba\n","    \n","    # 英文的大写全部改成小写\n","    #q_list_lower = sentence.lower()    \n","    # 把问题拆解成一个一个单词\n","    word_tokens = jieba.cut_for_search(sentence)\n","    \n","    # 过滤掉停用词\n","    ## 增加stopwords\n","    #stop_words = set(stopwords.words('chinese'))\n","    #stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n","    #filtered_words = [word for word in word_tokens if word not in stop_words]\n","   \n","    filtered_words =[word for word in word_tokens if word not in stopwords_list]\n","    #print(filtered_words)\n","\n","    # 将单词组合成句子\n","    filtered_sentens = ' '.join(filtered_words)\n","    q_filtered_list = [filtered_sentens]\n","    \n","    \n","    '''\n","    # 过滤掉停用词\n","    ## 增加stopwords\n","    stop_words = set(stopwords.words('english'))\n","    stop_words.update(['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}'])\n","    filtered_words = [word for word in word_tokens if word not in stop_words]\n","\n","    # 将单词组合成句子\n","    filtered_sentens = ' '.join(filtered_words)       \n","    q_filtered_list = [filtered_sentens]\n","    '''\n","    \n","    \n","    #time test\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime, 4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    return q_filtered_list , filtered_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Fv2QRRauO01"},"source":["#(3)透过倒排表搜索\n","def get_inverted_idx(q_total_word):\n","    \n","    #time test\n","    print(\"----------------get_inverted_idx-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","  \n","    \n","    \n","    \n","    import json\n","    # 读取word的倒排表字典.json\n","    with open('inverted_idx.json', 'r') as fp:\n","        inverted_idx_dic = json.load(fp)\n","  \n","    \n","    # 查找问题的index列表\n","    q_total_list = []\n","    for q_word in q_total_word:\n","    \n","        # 只找字典中有的单词进行查找\n","        if q_word in inverted_idx_dic:\n","            q_total_list += inverted_idx_dic[q_word]\n","\n","    # 清除重复的问题的index        \n","    q_total_clean_list = list(set(q_total_list))####\n","\n","\n","    #time test\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","\n","\n","    global col\n","    col+=1\n","    if workbook:\n","        table.cell(row=row_,column=col,value=str(timeelapsed))\n"," \n","\n","\n","    return q_total_clean_list\n","\n","\n","####\n","#(4) 透过index找出所有相对应的问题\n","def get_total_q(index_q):\n","    \n","    #time test\n","    print(\"----------------get_total_q-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","    \n","    \n","    \n","    # 读取question的word_total_list.json\n","    import json\n","    with open('clean_q_word_list.json','r') as flie:\n","        clean_q_word_list = flie.read()\n","        clean_q_list = json.loads(clean_q_word_list)\n","        \n","    #找到问题与问题集相似的所有问题\n","    total_q_list = []\n","    for q in index_q:\n","        total_q_list += [clean_q_list[q]]\n","    \n","    \n","    \n","    #time test\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    \n","    \n","    \n","    return total_q_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uI-eG133uR37"},"source":["#(5)执行步骤整理\n","#get_preprocess:藉由一個使用者問的問題，找到(衍生)58個可能相似的問題 \n","def get_preprocess(sentence):\n","    \n","    #time test\n","    #計算程序運行時間\n","    starttime = time.time()\n","  \n","    \n","    \n","    #1.去除停用詞 2.增加相似詞彙 3.倒排表查詢可能問題\n","    # 整理完的问题集\n","    clean_q , word_list = q_movestopwords(sentence)\n","    #print(clean_q)\n","    #print(word_list)\n","    # 相似词列表\n","    simi_list=word_list\n","    #simi_list = get_related_words(word_list)######\n","    # 倒排表找出所有可能的问题index\n","    q_simi_list = get_inverted_idx(simi_list)\n","    #print(\"q_simi_list:\",q_simi_list)\n","    \n","    # 透过index找出所有相对应的问题\n","    q_total_list = get_total_q(q_simi_list)\n","    #print(\"q_total_list:\",q_total_list)\n","    \n","    \n","    \n","    #time test\n","    print(\"----------------get_preprocess-----------------\")\n","    print(\"starttime:\",starttime)\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","   \n","\n","    return q_total_list , q_simi_list\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOO51O-1uT7N"},"source":["## Torch.no_Grad关闭梯度计算，节省内存，并加快计算速度\n","def get_encoded_layers(tokens_tensor,segments_tensors):\n","    \n","    \n","   \n","    \n","    \n","    import torch\n","    #from pytorch_pretrained_bert import BertTokenizer\n","    from pytorch_pretrained_bert import BertModel\n","    #from pytorch_pretrained_bert import BertForMaskedLM\n","    # 使用bert-base-chinese\n","    #tokenizer = BertTokenizer.from_pretrained('.\\model')#.\\model\n","    model = BertModel.from_pretrained('./model')\n","    #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","    #model = BertModel.from_pretrained('bert-base-uncased')\n","\n","    with torch.no_grad():\n","        encoded_layers, _ = model(tokens_tensor, segments_tensors)\n","        \n","    \n","    return(encoded_layers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6bpUJS1uWPd"},"source":["initial_numpy = np.zeros((768,)) \n","\n","#取得Bert 的句子向量并整理成array(模型之架構: 輸入層->隱藏層1->隱藏層2->.....->輸出層)\n","import numpy as np\n","import threading                   ##1\n","class MyThread(threading.Thread):   ##1\n","    def __init__(self,i,new_clean_q):\n","        threading.Thread.__init__(self)\n","        self.i, self.new_clean_q=i,new_clean_q\n","        \n","    def run(self):\n","        for sentence in self.new_clean_q:\n","            \n","            #print(\"sentence:\",sentence)\n","            # 句子的分词\n","            tokenized_text = get_tokenized_text(sentence)\n","            \n","            \n","            \n","            # BERT PyTorch要求张量要转成torch张量 Convert inputs to PyTorch tensors\n","            tokens_tensor, segments_tensors = get_tokens_segments_tensor(i,sentence)\n","    \n","            # 取得隐藏层层数\n","            encoded_layers = get_encoded_layers(tokens_tensor,segments_tensors)\n","    \n","            # 取得单词向量，取平均成句子的向量\n","            sentence_embedding = get_sentence_embedding(tokenized_text,encoded_layers)\n","        \n","            # 把torch张量转成array\n","            sentence_embedding_array = np.array(sentence_embedding[0])\n","            \n","            #array全部组合起来\n","            #lock.acquire()\n","            global initial_numpy         \n","            initial_numpy= np.vstack((initial_numpy,sentence_embedding_array))\n","            #print(\"initial_numpy:\",initial_numpy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eaUuQNkbuYa8"},"source":["def get_new_bert(new_clean_q):\n","#計算程序運行時間\n","    print(\"----------------get_new_bert-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","    \n","    #import numpy as np\n","    # 将句子拆成每一个词，将词向量平均建构句子的向量\n","    #X_bert = []\n","    import numpy as np\n","    #i = 0\n","    #initial_numpy = np.zeros((768,))\n","    #new_clean=new_clean_q\n","    new_clean_q=np.array(new_clean_q)\n","    \n","    \n","\n","    \n","    t5=[]\n","    seperated_array=np.array_split(new_clean_q,3)\n","    for i in range(3):\n","\n","        tn=MyThread(i,seperated_array[i])\n","        #print(\"seperated_array[i]:\",seperated_array[i])\n","        t5.append(tn)\n","    #print(\"t5:\",t5)\n","    \n","    for j in t5:\n","        j.start()\n","        j.join()\n","    \n","    #t5[0].start()\n","    #print(\"initial_numpy:\",initial_numpy)\n","    bert_vectorizer = initial_numpy\n","    #print(\"bert_vectorizer:\",bert_vectorizer)\n","    #print(\"len(bert_vectorizer):\",len(bert_vectorizer))\n","    \n","    global col\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"\\nendtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    return bert_vectorizer\n","print(\"ok!!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cX2gcrC7ud2V"},"source":["#(7)整理成餘弦函數輸入的格式 (這裡我一共整理成3種文本表示,前兩篇文章為介紹TF-IDF、Word2Vec有興趣的小伙伴可以找一下)\n","# 输入tfidf、word2vec、bert资料、1次batch的问题量、是哪一种函数(tfidf、word2vec、bert)\n","def get_cos_list(new_q_list, q_list_batch, model_type):\n","\n","    #計算程序運行時間\n","    print(\"----------------get_cos_list-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","\n","    \n","    import numpy as np\n","    \n","    #设定1000笔作batch\n","    num = 0\n","    add_num = q_list_batch\n","    \n","    #设定空array\n","    cos_array = np.empty(shape=[0, 1])\n","    \n","    # 输入的资料依照是哪一种函数整理成array(分别为询问的问题、全部的问题、共有多少题)\n","    new_q_array , ask_q_array , array_length = get_arry(new_q_list, model_type)\n","    #print(\"new_q_array:\",new_q_array)\n","\n","    while num< array_length:\n","        if num + add_num  < array_length:\n","            cos_distance_array = get_cos_distance(ask_q_array, new_q_array[num : num + add_num])\n","            cos_array = np.append(cos_array , cos_distance_array)\n","        else:\n","            cos_distance_array = get_cos_distance(ask_q_array, new_q_array[num : ])\n","            cos_array = np.append(cos_array , cos_distance_array)\n","            \n","        num += add_num\n","    #print(\"cos_distant:\",cos_array)\n","    \n","    \n","\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    return cos_array   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXrExcYyugmQ"},"source":["#(8)Bert整理成array\n","def get_arry(data, model_type):\n","\n","    if model_type == 'bert':\n","        import numpy as np\n","        \n","        new_array = np.array(data)\n","        ask_q_array = np.array(data[-1:])\n","        length = len(data)\n","\n","    else:\n","        print('no input model')\n","    \n","    return new_array,ask_q_array,length\n","\n","#(9)余弦函数计算\n","def get_cos_distance(X1, X2):\n","    \n","    import tensorflow as tf\n","    #import numpy as np    \n","    \n","    #sess=tf.Session()\n","    with tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n","        with tf.device('/gpu:0'):\n","            # calculate cos distance between two sets\n","            # more similar more big\n","    \n","            # 新增询问的问题\n","            X1_matrix = X1\n","            \n","            # 全部的问题\n","            X2_matrix = X2\n","          \n","            \n","            # 计算矩阵型态\n","            (k,n) = X1_matrix.shape\n","            (m,n) = X2_matrix.shape\n","        with tf.device('/gpu:0'):\n","            \n","    \n","            # 求模\n","            X1_norm = sess.run(tf.sqrt(tf.reduce_sum(tf.square(X1_matrix), axis=1)))\n","            X2_norm = sess.run(tf.sqrt(tf.reduce_sum(tf.square(X2_matrix), axis=1)))\n","            # 内积\n","            X1_X2_muti = sess.run(tf.matmul(X1_matrix, tf.transpose(X2_matrix)))\n","            X1_X2_norm = sess.run(tf.matmul(tf.reshape(X1_norm,[k,1]),tf.reshape(X2_norm,[1,m])))\n","            # 计算余弦距离\n","            #cos_dis = get_cos_distance(X1_X2_muti, X1_X2_norm )\n","            \n","            cos_dis = sess.run(tf.math.divide(X1_X2_muti, X1_X2_norm))\n","            #cos = cos_dis.eval(session=session)\n","\n","    sess.close()\n","    return cos_dis\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mGot0JAul2D"},"source":["#(10)找出前index_num個最相似的問题，返回前58個相似問題的index。index_num個最相似的問题<->前58個相似問題中對應的index\n","def get_top_index(cos_array,index_num):\n"," \n","    #計算程序運行時間\n","    print(\"----------------get_top_index-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","    \n","    \n","    import heapq\n","    # array转list\n","    cos_list = cos_array.tolist()\n","    \n","    # 最大的索引个数(因为list最后就是新增的问题要去掉,所以要多取1个)\n","    #因為尾巴新增的問題(使用者問得問題)一定是數值最高的(100%=1符合)，故最大索引個數要加1\n","    top_num_count = index_num+1\n","    #map(cos_list.index, heapq.nlargest(top_num_count, cos_list)):\n","    #利用cos_list.index(heapq.nlargest(top_num_count, cos_list))分別找出\n","    #前index_num對應數值之index\n","    max_index =map(cos_list.index, heapq.nlargest(top_num_count, cos_list))#max_index=[相對應index, 相對應index]\n","    #print(max_index)\n","    #print(\"heapq.nlargest(top_num_count, cos_list):\",heapq.nlargest(top_num_count, cos_list))\n","    #heapq.nlargest(n, iterable, key=None):\n","    #从 iterable 所定义的数据集中返回前 n 个最大元素组成的列表。 如\n","    #果提供了 key 则其应指定一个单参数的函数，用于从 iterable 的每个\n","    #元素中提取比较键 (例如 key=str.lower)。 等价于: \n","    #sorted(iterable, key=key, reverse=True)[:n]。\n","    \n","    #产生成list\n","    top_index_list = list(max_index)\n","    top_index = top_index_list\n","    #print(\"top_idxs:\",top_index)\n","    \n","    \n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime,4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    \n","    return top_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t2pWevqMuoOZ"},"source":["#(11)找出相对应最有可能的答案\n","def get_answer(a_index , q_index):\n","\n","    \n","    #計算程序運行時間\n","    print(\"----------------get_answer-----------------\")\n","    starttime = time.time()\n","    print(\"starttime:\",starttime)\n","    \n","    \n","    \n","    import json\n","    #print(\"time now:\",time.time())\n","    with open('a_list.json', 'r') as file:\n","        answers = file.read()\n","        alist = json.loads(answers)\n","    \n","    q_index_list = []\n","    #print(q_index_list)\n","    #从新建立好的倒排表list反推可能问题list\n","    for u in a_index:   #q_index不包括新增的問題(使用者問得問題)，len=58\n","        if u < len(q_index):  #如果前index_num個相似問題之index在前58個相似問題之\n","                              #長度內，當超過範圍時(第59個:使用者問的問題)，即不加入\n","            q_index_list += [q_index[u]]\n","    #print(\"q_index_list:\",q_index_list)\n","    \n","\n","    #从可能问题的list找到相对应的答案list\n","    top_a = []\n","    \n","    for i in q_index_list:\n","        top_a += [alist[i]]\n","    #print(\"alist:\",top_a)\n","    \n","    \n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime, 4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    \n","    \n","    return top_a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ARP5WhLCuqH-"},"source":["#(12)执行Bert的步骤整理表\n","#@jit()\n","def get_top_results_bert(query):\n","    #計算程序運行時間\n","    starttime = time.time()\n","    \n","    \n","    \n","    # top_idxs存放相似度最高的（存在qlist里的）问题的下表 \n","    # hint: 利用priority queue来找出top results. 思考为什么可以这么做？ \n","    \n","    \"\"\"\n","    给定用户输入的问题 query, 返回最有可能的TOP 5问题。这里面需要做到以下几点：\n","    1. 利用倒排表来筛选 candidate （需要使用related_words). \n","    2. 对于候选文档，计算跟输入问题之间的相似度\n","    3. 找出相似度最高的top5问题的答案\n","    \"\"\"\n","    # 找出与问题与问题集中相似的问题\n","    preprocess_q, q_index_list = get_preprocess(query) #preprocess_q=[,,,,可能的問題(文字)]   q_index_list=[,,,可能的問題的index]\n","    # 建立新的bert的句子词向量(单词向量平均)\n","    q_new_list = preprocess_q + [query]  #加入使用者問的問題，len長度變59\n","    new_bert = get_new_bert(q_new_list)\n","    #取得新问题对问题集的Bert余弦函数\n","    cos_distant = get_cos_list(new_bert, q_list_batch = 1000, model_type = 'bert')\n","    # 取得问题在问题集中最相近的5个问题\n","    top_idxs = get_top_index(cos_distant,5)\n","\n","    #取得最接近的答案list\n","    alist = get_answer(top_idxs , q_index_list)\n","\n","    ##显示答案\n","    #for top_a in alist:\n","        #print(top_a) \n","        \n","    global row_\n","    print(\"----------------get_top_results_bert-----------------\")\n","    print(\"starttime:\",starttime)\n","    endtime = time.time()\n","    timeelapsed=round(endtime-starttime, 4)\n","    print(\"endtime:\",endtime)\n","    print('time elapsed: ' , timeelapsed , ' seconds\\n')\n","    \n","    global col\n","    col+=1\n","    table.cell(row=row_,column=col,value=str(timeelapsed))\n","    \n","    #row_+=1\n","    #col=1\n","\n","    return alist  # 返回相似度最高的问题对应的答案，作为TOP5答案\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OyDTczoIwoDY"},"source":["\n","#c=0\n","col =1\n","row_=2\n","workbook = None\n","if(workbook==None):  #建立xlsx檔\n","    write_excel_xlsx(\"testGPUV4.xlsx\",\"test_table\",\n","                             [[\"Question\",\"q_movestopwords\",\"get_inverted_idx\",\"get_total_q\",\n","                              \"get_preprocess\",\"get_new_bert\",\"get_cos_list\",\"get_top_index\",\n","                              \"get_answer\",\"總共\",\"正確答案\"]])\n","workbook = openpyxl.load_workbook(\"testGPUV4.xlsx\") #載入xlsx檔\n","sheetnames = workbook.get_sheet_names()\n","table=workbook.get_sheet_by_name(sheetnames[0])\n","table=workbook.active\n","table.cell(row=row_,column=col,value=\"廣州的快速公交運輸系統每多久就會有一輛巴士？\")\n","\n","answer=get_top_results_bert(\"廣州的快速公交運輸系統每多久就會有一輛巴士？\")\n","print ('bert:',answer)\n","print('answer:',answer[0])\n","#workbook.save('testGPUV4.xlsx')"],"execution_count":null,"outputs":[]}]}